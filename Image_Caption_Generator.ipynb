{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Image Caption Generator.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_flOwMTv5tC",
        "outputId": "36be2a84-01b9-4677-fd5c-bc2c3541aeea"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zSTBkk3vr0T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "072f7125-e805-4f54-cd60-80fc508ae6a2"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import string\n",
        "import os\n",
        "from PIL import Image\n",
        "import glob\n",
        "from pickle import dump, load\n",
        "from time import time\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
        "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers.merge import add\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras import Input, layers\n",
        "from keras import optimizers\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLoUR0U2wAaE",
        "outputId": "6b83203b-08e8-425e-ebdd-99f6f8627850"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "filename = \"/content/drive/MyDrive/Image Caption Generator/Flickr8k_text/Flickr8k.token.txt\"\n",
        "# load descriptions\n",
        "doc = load_doc(filename)\n",
        "print(doc[:300])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .\n",
            "1000268201_693b08cb0e.jpg#1\tA girl going into a wooden building .\n",
            "1000268201_693b08cb0e.jpg#2\tA little girl climbing into a wooden playhouse .\n",
            "1000268201_693b08cb0e.jpg#3\tA little girl climbing the s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_XuzPkTwfgu",
        "outputId": "2daf470b-94b9-48fa-fbb9-8497eafc4397"
      },
      "source": [
        "def load_descriptions(doc):\n",
        "\tmapping = dict()\n",
        "\t# process lines\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\t# take the first token as the image id, the rest as the description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# extract filename from image id\n",
        "\t\timage_id = image_id.split('.')[0]\n",
        "\t\t# convert description tokens back to string\n",
        "\t\timage_desc = ' '.join(image_desc)\n",
        "\t\t# create the list if needed\n",
        "\t\tif image_id not in mapping:\n",
        "\t\t\tmapping[image_id] = list()\n",
        "\t\t# store description\n",
        "\t\tmapping[image_id].append(image_desc)\n",
        "\treturn mapping\n",
        "\n",
        "# parse descriptions\n",
        "descriptions = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded: 8092 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDks6fnZwlBk",
        "outputId": "c3ee4c7e-214e-4673-a9e4-76bfad4141ea"
      },
      "source": [
        "list(descriptions.keys())[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1000268201_693b08cb0e',\n",
              " '1001773457_577c3a7d70',\n",
              " '1002674143_1b742ab4b8',\n",
              " '1003163366_44323f5815',\n",
              " '1007129816_e794419615']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ak1bJCPwmIC",
        "outputId": "4baf9575-7ab4-4c65-ad34-603d0324f2ff"
      },
      "source": [
        "descriptions['1000268201_693b08cb0e']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
              " 'A girl going into a wooden building .',\n",
              " 'A little girl climbing into a wooden playhouse .',\n",
              " 'A little girl climbing the stairs to her playhouse .',\n",
              " 'A little girl in a pink dress going into a wooden cabin .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhvvlYaKwph4",
        "outputId": "a0815f58-429d-472c-e7b7-9d5ecff81b06"
      },
      "source": [
        "descriptions['1001773457_577c3a7d70']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A black dog and a spotted dog are fighting',\n",
              " 'A black dog and a tri-colored dog playing with each other on the road .',\n",
              " 'A black dog and a white dog with brown spots are staring at each other in the street .',\n",
              " 'Two dogs of different breeds looking at each other on the road .',\n",
              " 'Two dogs on pavement moving toward each other .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5yz2OI6wtZe"
      },
      "source": [
        "def clean_descriptions(descriptions):\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor i in range(len(desc_list)):\n",
        "\t\t\tdesc = desc_list[i]\n",
        "\t\t\t# tokenize\n",
        "\t\t\tdesc = desc.split()\n",
        "\t\t\t# convert to lower case\n",
        "\t\t\tdesc = [word.lower() for word in desc]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tdesc = [w.translate(table) for w in desc]\n",
        "\t\t\t# remove hanging 's' and 'a'\n",
        "\t\t\tdesc = [word for word in desc if len(word)>1]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tdesc = [word for word in desc if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tdesc_list[i] =  ' '.join(desc)\n",
        "\n",
        "# clean descriptions\n",
        "clean_descriptions(descriptions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hys-TwfvwxqR",
        "outputId": "eef50205-402f-456b-bfcc-3bd0a7b4d5aa"
      },
      "source": [
        "descriptions['1000268201_693b08cb0e']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['child in pink dress is climbing up set of stairs in an entry way',\n",
              " 'girl going into wooden building',\n",
              " 'little girl climbing into wooden playhouse',\n",
              " 'little girl climbing the stairs to her playhouse',\n",
              " 'little girl in pink dress going into wooden cabin']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHsZ5uvTw4By",
        "outputId": "f789c7c5-152f-4ab1-c185-46fc87f49c5c"
      },
      "source": [
        "descriptions['1001773457_577c3a7d70']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['black dog and spotted dog are fighting',\n",
              " 'black dog and tricolored dog playing with each other on the road',\n",
              " 'black dog and white dog with brown spots are staring at each other in the street',\n",
              " 'two dogs of different breeds looking at each other on the road',\n",
              " 'two dogs on pavement moving toward each other']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoNY8Tx0w688",
        "outputId": "470f682a-96ce-430a-e664-d5952177d2a6"
      },
      "source": [
        "# convert the loaded descriptions into a vocabulary of words\n",
        "def to_vocabulary(descriptions):\n",
        "\t# build a list of all description strings\n",
        "\tall_desc = set()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        "\n",
        "# summarize vocabulary\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "print('Original Vocabulary Size: %d' % len(vocabulary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Vocabulary Size: 8763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfQGrVbdw90f"
      },
      "source": [
        "# save descriptions to file, one per line\n",
        "def save_descriptions(descriptions, filename):\n",
        "\tlines = list()\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\tlines.append(key + ' ' + desc)\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        "\n",
        "save_descriptions(descriptions, 'descriptions.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn7eRisaxCBX",
        "outputId": "6866d977-1056-4663-f368-cf1226883b50"
      },
      "source": [
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\t# process line by line\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# skip empty lines\n",
        "\t\tif len(line) < 1:\n",
        "\t\t\tcontinue\n",
        "\t\t# get the image identifier\n",
        "\t\tidentifier = line.split('.')[0]\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)\n",
        "\n",
        "# load training dataset (6K)\n",
        "filename = '/content/drive/MyDrive/Image Caption Generator/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxa8Da09xOix",
        "outputId": "f14d5925-3481-4662-8e58-cec04e7983c1"
      },
      "source": [
        "# Below path contains all the images\n",
        "images = '/content/drive/MyDrive/Image Caption Generator/Flicker8k_Dataset/'\n",
        "# Create a list of all image names in the directory\n",
        "img = glob.glob(images + '*.jpg')\n",
        "len(img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8091"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHlW__YHxWly"
      },
      "source": [
        "# Below file conatains the names of images to be used in train data\n",
        "train_images_file = '/content/drive/MyDrive/Image Caption Generator/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "# Read the train image names in a set\n",
        "train_images = set(open(train_images_file, 'r').read().strip().split('\\n'))\n",
        "\n",
        "# Create a list of all the training images with their full path names\n",
        "train_img = []\n",
        "\n",
        "for i in img: # img is list of full path names of all images\n",
        "    if i[len(images):] in train_images: # Check if the image belongs to training set\n",
        "        train_img.append(i) # Add it to the list of train images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56qEOViXRXvC",
        "outputId": "ca619c3e-65cf-4481-fcf0-6ef87cfa3cfc"
      },
      "source": [
        "len(train_img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpw-Nyg2xckG"
      },
      "source": [
        "# Below file conatains the names of images to be used in test data\n",
        "test_images_file = '/content/drive/MyDrive/Image Caption Generator/Flickr8k_text/Flickr_8k.testImages.txt'\n",
        "# Read the validation image names in a set# Read the test image names in a set\n",
        "test_images = set(open(test_images_file, 'r').read().strip().split('\\n'))\n",
        "\n",
        "# Create a list of all the test images with their full path names\n",
        "test_img = []\n",
        "\n",
        "for i in img: # img is list of full path names of all images\n",
        "    if i[len(images):] in test_images: # Check if the image belongs to test set\n",
        "        test_img.append(i) # Add it to the list of test images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2da1nfjRV8Y",
        "outputId": "eadac794-0da5-4486-cb20-728acea7d187"
      },
      "source": [
        "len(test_img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOtKdefUxilI",
        "outputId": "5add7e2f-3104-484c-a360-fc582eedc889"
      },
      "source": [
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "\t# load document\n",
        "\tdoc = load_doc(filename)\n",
        "\tdescriptions = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\t# split id from description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# skip images not in the set\n",
        "\t\tif image_id in dataset:\n",
        "\t\t\t# create list\n",
        "\t\t\tif image_id not in descriptions:\n",
        "\t\t\t\tdescriptions[image_id] = list()\n",
        "\t\t\t# wrap description in tokens\n",
        "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "\t\t\t# store\n",
        "\t\t\tdescriptions[image_id].append(desc)\n",
        "\treturn descriptions\n",
        "\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Descriptions: train=6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFoxnL4dxosZ"
      },
      "source": [
        "def preprocess(image_path):\n",
        "    # Convert all the images to size 299x299 as expected by the inception v3 model\n",
        "    img = image.load_img(image_path, target_size=(299, 299))\n",
        "    # Convert PIL image to numpy array of 3-dimensions\n",
        "    x = image.img_to_array(img)\n",
        "    # Add one more dimension\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    # preprocess the images using preprocess_input() from inception module\n",
        "    x = preprocess_input(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjWzKvnZxqD_",
        "outputId": "4b68ce0e-23e3-465d-8fff-e42786e77ac7"
      },
      "source": [
        "# Load the inception v3 model\n",
        "model = InceptionV3(weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "96116736/96112376 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik9qVRqxxwXk"
      },
      "source": [
        "# Create a new model, by removing the last layer (output layer) from the inception v3\n",
        "model_new = Model(model.input, model.layers[-2].output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPDFRu3Nxzy6"
      },
      "source": [
        "# Function to encode a given image into a vector of size (2048, )\n",
        "def encode(image):\n",
        "    image = preprocess(image) # preprocess the image\n",
        "    fea_vec = model_new.predict(image) # Get the encoding vector for the image\n",
        "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1]) # reshape from (1, 2048) to (2048, )\n",
        "    return fea_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfNQpR9Vx04w",
        "outputId": "d84b4140-3c1b-412e-bda0-4cea0d0ee390"
      },
      "source": [
        "# Call the funtion to encode all the train images\n",
        "# This will take a while on CPU - Execute this only once\n",
        "start = time()\n",
        "encoding_train = {}\n",
        "for img in train_img:\n",
        "    encoding_train[img[len(images):]] = encode(img)\n",
        "print(\"Time taken in seconds =\", time()-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken in seconds = 344.5886809825897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksoFbD9Lx6Sv"
      },
      "source": [
        "# Save the bottleneck train features to disk\n",
        "with open(\"/content/drive/MyDrive/Image Caption Generator/encoded_train_images.pkl\", \"wb\") as encoded_pickle:\n",
        "    dump(encoding_train, encoded_pickle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b611AQXHylFg",
        "outputId": "99a152c1-1f7d-4391-ab60-4b1dffb84499"
      },
      "source": [
        "# Call the funtion to encode all the test images - Execute this only once\n",
        "start = time()\n",
        "encoding_test = {}\n",
        "for img in test_img:\n",
        "    encoding_test[img[len(images):]] = encode(img)\n",
        "print(\"Time taken in seconds =\", time()-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken in seconds = 56.35714650154114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVRZHV6ayqA-"
      },
      "source": [
        "# Save the bottleneck test features to disk\n",
        "with open(\"/content/drive/MyDrive/Image Caption Generator/encoded_test_images.pkl\", \"wb\") as encoded_pickle:\n",
        "    dump(encoding_test, encoded_pickle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-utHtINyzD3",
        "outputId": "fac1fbf5-aee2-4c76-e64c-cad5eff5ad7a"
      },
      "source": [
        "train_features = load(open(\"/content/drive/MyDrive/Image Caption Generator/encoded_train_images.pkl\", \"rb\"))\n",
        "print('Photos: train=%d' % len(train_features))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Photos: train=6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiCTlr3-FAiV",
        "outputId": "bb9a5892-28db-4ad6-98dd-8b55bc7db20b"
      },
      "source": [
        "f = open('flickr8k_training_dataset.txt', 'w')\n",
        "f.write(\"image_id\\tcaptions\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCP_BWyYFTab"
      },
      "source": [
        "for key, val in train_descriptions.items():\n",
        "    for i in val:\n",
        "        f.write(key[len(images):] + \"\\t\" + \"<start> \" + i +\" <end>\" + \"\\n\")\n",
        "\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWh2mTTF7ih"
      },
      "source": [
        "df = pd.read_csv('flickr8k_training_dataset.txt', delimiter='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "tJJB0-IsGUSs",
        "outputId": "83e20e1c-3a79-425b-a1d6-0c9dd5b80f36"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>captions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;start&gt; startseq child in pink dress is climbi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;start&gt; startseq girl going into wooden buildi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;start&gt; startseq little girl climbing into woo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;start&gt; startseq little girl climbing the stai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;start&gt; startseq little girl in pink dress goi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29995</th>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;start&gt; startseq person stands near golden wal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29996</th>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;start&gt; startseq woman behind scrolled wall is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29997</th>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;start&gt; startseq woman standing near decorated...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29998</th>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;start&gt; startseq the walls are covered in gold...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29999</th>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;start&gt; startseq woman writing on pad in room ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       image_id                                           captions\n",
              "0           NaN  <start> startseq child in pink dress is climbi...\n",
              "1           NaN  <start> startseq girl going into wooden buildi...\n",
              "2           NaN  <start> startseq little girl climbing into woo...\n",
              "3           NaN  <start> startseq little girl climbing the stai...\n",
              "4           NaN  <start> startseq little girl in pink dress goi...\n",
              "...         ...                                                ...\n",
              "29995       NaN  <start> startseq person stands near golden wal...\n",
              "29996       NaN  <start> startseq woman behind scrolled wall is...\n",
              "29997       NaN  <start> startseq woman standing near decorated...\n",
              "29998       NaN  <start> startseq the walls are covered in gold...\n",
              "29999       NaN  <start> startseq woman writing on pad in room ...\n",
              "\n",
              "[30000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6voonMnF8nI",
        "outputId": "300ed3be-c95f-461c-b219-e1062059bebb"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mRgrHREGE9R",
        "outputId": "acfb86c7-84bd-4e0a-f34c-828fc00d1507"
      },
      "source": [
        "c = [i for i in df['captions']]\n",
        "len(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LpNqOhJGGQs",
        "outputId": "7d1f43d4-490d-4e49-cbfe-3453afd8136e"
      },
      "source": [
        "imgs = [i for i in df['image_id']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWMpuaLTGKEO",
        "outputId": "730666e6-def1-40b9-f8f3-19d5cde1f05f"
      },
      "source": [
        "a = c[-1]\n",
        "a, imgs[-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('<start> startseq woman writing on pad in room with gold decorated walls endseq <end>',\n",
              " nan)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KGRdV9Ofr-y",
        "outputId": "84cbd9de-1fd0-49f3-97b2-10e6e803f60a"
      },
      "source": [
        "# Create a list of all the training captions\n",
        "all_train_captions = []\n",
        "for key, val in train_descriptions.items():\n",
        "    for cap in val:\n",
        "        all_train_captions.append(cap)\n",
        "len(all_train_captions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUe5MBSdf0h9",
        "outputId": "f562037b-3303-4653-f24c-a916ea711800"
      },
      "source": [
        "# Consider only words which occur at least 10 times in the corpus\n",
        "word_count_threshold = 10\n",
        "word_counts = {}\n",
        "nsents = 0\n",
        "for sent in all_train_captions:\n",
        "    nsents += 1\n",
        "    for w in sent.split(' '):\n",
        "        word_counts[w] = word_counts.get(w, 0) + 1\n",
        "\n",
        "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
        "print('preprocessed words %d -> %d' % (len(word_counts), len(vocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preprocessed words 7578 -> 1651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNP9h_Ipf6pw"
      },
      "source": [
        "ixtoword = {}\n",
        "wordtoix = {}\n",
        "\n",
        "ix = 1\n",
        "for w in vocab:\n",
        "    wordtoix[w] = ix\n",
        "    ixtoword[ix] = w\n",
        "    ix += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnES_nSAf750",
        "outputId": "914c66cc-40cf-4742-a960-91db34a7ceaf"
      },
      "source": [
        "vocab_size = len(ixtoword) + 1 # one for appended 0's\n",
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1652"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXNSgm1hf-bC",
        "outputId": "c864f900-7706-4cdd-ffda-c315447893a3"
      },
      "source": [
        "# convert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        "\n",
        "# calculate the length of the description with the most words\n",
        "def max_length(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\treturn max(len(d.split()) for d in lines)\n",
        "\n",
        "# determine the maximum sequence length\n",
        "max_length = max_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Description Length: 34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1mAUx5ugFH4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1b91193-81cf-4ced-a106-99cc05d96aba"
      },
      "source": [
        "# Load Glove vectors\n",
        "glove_dir = '/content/drive/MyDrive/Image Caption Generator/'\n",
        "embeddings_index = {} # empty dictionary\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q6qKc_OmiCe"
      },
      "source": [
        "embedding_dim = 200\n",
        "\n",
        "# Get 200-dim dense vector for each of the 10000 words in out vocabulary\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, i in wordtoix.items():\n",
        "    #if i < max_words:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in the embedding index will be all zeros\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogr7XtvGmll4",
        "outputId": "861c6c07-0b8e-43da-857c-15382a0697d9"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1652, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "T-8b452TmqCe",
        "outputId": "10e84503-7474-48b7-f5a5-9ccf758df4aa"
      },
      "source": [
        "inputs1 = Input(shape=(2048,))\n",
        "fe1 = Dropout(0.2)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.2)(se1)\n",
        "se3 = LSTM(256, return_sequences = True)(se2)\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6b24a8a53cd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfe1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfe2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfe1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minputs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mse1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_zero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_JsnCRumujr"
      },
      "source": [
        "model.layers[2].set_weights([embedding_matrix])\n",
        "model.layers[2].trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E2klWAAmynL"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McqTEe55m1eV"
      },
      "source": [
        "epochs = 10\n",
        "number_pics_per_bath = 3\n",
        "steps = len(train_descriptions)//number_pics_per_bath"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZmAcsTigA-6"
      },
      "source": [
        "# # data generator, intended to be used in a call to model.fit_generator()\n",
        "# def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
        "#     X1 = []\n",
        "#     X2 = []\n",
        "#     y = []\n",
        "#     n=0\n",
        "#     # loop for ever over images\n",
        "#     while 1:\n",
        "#         for key, desc_list in descriptions.items():\n",
        "#             n+=1\n",
        "#             # retrieve the photo feature\n",
        "#             photo = photos[key+'.jpg']\n",
        "#             for desc in desc_list:\n",
        "#                 # encode the sequence\n",
        "#                 seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
        "#                 # split one sequence into multiple X, y pairs\n",
        "#                 for i in range(1, len(seq)):\n",
        "#                     # split into input and output pair\n",
        "#                     in_seq, out_seq = seq[:i], seq[i]\n",
        "#                     # pad input sequence\n",
        "#                     in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "#                     # encode output sequence\n",
        "#                     out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "#                     # store\n",
        "#                     X1.append(photo)\n",
        "#                     X2.append(in_seq)\n",
        "#                     y.append(out_seq)\n",
        "#             # yield the batch data\n",
        "#             if n==num_photos_per_batch:\n",
        "#                 #yield [[array(X1), array(X2)], array(y)]\n",
        "#                 yield [[np.array(X1), np.array(X2)], np.array(y)]\n",
        "#                 X1, X2, y = list(), list(), list()\n",
        "#                 n=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "L81PC8WWm5Pu",
        "outputId": "e5150232-9fe1-4d3c-a6c4-14ec4b72504f"
      },
      "source": [
        "for i in range(epochs):\n",
        "    generator = data_generator(batch_size=128)\n",
        "    model.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1)\n",
        "    model.save('/content/drive/MyDrive/Image Caption Generator/Model/' + str(i) + '.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-f062e503f548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Image Caption Generator/Model/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1061\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[0;31m# Since we have to know the dtype of the python generator when we build the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[0;31m# dataset, we have to look at a batch to infer the structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m     \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_peek_and_restore\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    841\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-95e6bd2b4933>\u001b[0m in \u001b[0;36mdata_generator\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_fallback_to_positional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAjp_rPMCS_H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzqxBxJKCTSc"
      },
      "source": [
        "def data_generator(batch_size = 32):\n",
        "        partial_caps = []\n",
        "        next_words = []\n",
        "        images = []\n",
        "        \n",
        "        df = pd.read_csv('/content/drive/MyDrive/Image Caption Generator/Flickr8k_text/Flickr_8k.trainImages.txt', delimiter='\\t')\n",
        "        df = df.sample(frac=1)\n",
        "        iter = df.iterrows()\n",
        "        c = []\n",
        "        imgs = []\n",
        "        for i in range(df.shape[0]):\n",
        "            x = next(iter)\n",
        "            c.append(x[1][1])\n",
        "            imgs.append(x[1][0])\n",
        "\n",
        "\n",
        "        count = 0\n",
        "        while True:\n",
        "            for j, text in enumerate(c):\n",
        "                current_image = encoding_train[imgs[j]]\n",
        "                for i in range(len(text.split())-1):\n",
        "                    count+=1\n",
        "                    \n",
        "                    partial = [word2idx[txt] for txt in text.split()[:i+1]]\n",
        "                    partial_caps.append(partial)\n",
        "                    \n",
        "                    # Initializing with zeros to create a one-hot encoding matrix\n",
        "                    # This is what we have to predict\n",
        "                    # Hence initializing it with vocab_size length\n",
        "                    n = np.zeros(vocab_size)\n",
        "                    # Setting the next word to 1 in the one-hot encoded matrix\n",
        "                    n[word2idx[text.split()[i+1]]] = 1\n",
        "                    next_words.append(n)\n",
        "                    \n",
        "                    images.append(current_image)\n",
        "\n",
        "                    if count>=batch_size:\n",
        "                        next_words = np.asarray(next_words)\n",
        "                        images = np.asarray(images)\n",
        "                        partial_caps = sequence.pad_sequences(partial_caps, maxlen=max_len, padding='post')\n",
        "                        yield [[images, partial_caps], next_words]\n",
        "                        partial_caps = []\n",
        "                        next_words = []\n",
        "                        images = []\n",
        "                        count = 0"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}